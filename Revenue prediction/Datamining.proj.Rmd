---
title: "Revenue Prediction model"
author: "Garima Sood"
date: "March 12, 2018"
output: html_document
---

```{r}
dataPath <- "C:/Users/garim/Documents/Quarter 2/Data Mining/Project/Merged data"
master_data <- read.csv(paste(dataPath,"master_data_with_imputed_budget_and_revenue.csv", sep = "/"))
summary(master_data)
```

**Data Processsing**

```{r}
master_data$release_date <- as.Date(master_data$release_date)

#To cut the impact of inflation on movie revenues & budgets, I am excluding data of movies released before Jan 1985

master_data <- master_data[master_data$release_date > as.Date("01/01/1985","%m/%d/%Y"),]
master_data <- master_data[master_data$budget > 0,]
master_data$actor_1_gender <- as.factor(ifelse(master_data$actor_1_gender==0,NA,ifelse(master_data$actor_1_gender==2,1,0)))
master_data$actor_2_gender <- as.factor(ifelse(master_data$actor_2_gender==0,NA,ifelse(master_data$actor_2_gender==2,1,0)))
master_data$actor_3_gender <- as.factor(ifelse(master_data$actor_3_gender==0,NA,ifelse(master_data$actor_3_gender==2,1,0)))
master_data$actor_4_gender <- as.factor(ifelse(master_data$actor_4_gender==0,NA,ifelse(master_data$actor_4_gender==2,1,0)))
master_data$actor_5_gender <- as.factor(ifelse(master_data$actor_5_gender==0,NA,ifelse(master_data$actor_5_gender==2,1,0)))
master_data$director_gender <- as.factor(ifelse(master_data$director_gender==0,NA,ifelse(master_data$director_gender==2,1,0)))
master_data$producer_gender <- as.factor(ifelse(master_data$producer_gender==0,NA,ifelse(master_data$producer_gender==2,1,0)))
master_data$collection <- as.factor(ifelse(nchar(as.character(master_data$belongs_to_collection))>0,1,0))

master_data$num_prod_comp <-(master_data$production_company_1!="")+(master_data$production_company_2!="")+
                            (master_data$production_company_3!="")

master_data$num_prod_ctry <-(master_data$production_country_1!="")+(master_data$production_country_2!="")+
                            (master_data$production_country_3!="")

library(lubridate)
master_data$release_month <- month.abb[month(master_data$release_date)]

master_data <- master_data[ , -which(names(master_data) %in% 
              c( "movie_id" ,"actor_1_name","actor_2_name","actor_3_name","actor_4_name","actor_5_name","director_name","producer_name",
                 "casting_gender","casting_name","belongs_to_collection","genre_2","genre_3","genre_4","production_company_1",
                 "production_company_2","production_company_3" ,"production_country_1", "production_country_2",  "production_country_3" , "spoken_language_1","spoken_language_2", "spoken_language_3" ,"homepage","imdb_id" ,"original_title","overview","poster_path", "status","title","video"))]

require(ggplot2)

p1<- ggplot(master_data, aes(x = actor_1_gender)) + geom_bar()
p2<- ggplot(master_data, aes(x = actor_2_gender)) + geom_bar()
p3<- ggplot(master_data, aes(x = actor_3_gender)) + geom_bar()
p4<- ggplot(master_data, aes(x = actor_4_gender)) + geom_bar()
p5<- ggplot(master_data, aes(x = actor_5_gender)) + geom_bar()
p6<- ggplot(master_data, aes(x = director_gender)) + geom_bar()
p7<- ggplot(master_data, aes(x = producer_gender)) + geom_bar()
p8<- ggplot(master_data, aes(x = budget)) + geom_histogram()
p9<- ggplot(master_data, aes(x = revenue)) + geom_histogram()
p10<-ggplot(master_data, aes(x = popularity)) + geom_histogram()
p11<-ggplot(master_data, aes(x = vote_average)) + geom_histogram()
p12<-ggplot(master_data, aes(x = vote_count)) + geom_histogram()

library(gridExtra)

grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12, nrow = 4, ncol=3)
```

Plots show that there are a lot of NA values in the different columns. Counting the NA values per column in the data

```{r}
perc_na <- function(x){
  return(sum(is.na(x))/length(x))
}

round(apply(master_data, 2, function(x) perc_na(x)),2)

master_data$na_count <- apply(master_data, 1, function(x) sum(is.na(x)))
table(master_data$na_count)
```

Deleting records with missing data in more than 9 columns, and checking the poportion of missing values in the updated data set

```{r}
data <- master_data[master_data$na_count<9,]
dim(data)

round(apply(data, 2, function(x) perc_na(x)),2)
```

After removing thee records, we are left with about 10% missing values in the gender column of lead actors, and 24% missing values in budget. Rest of the columns look good. 

Checking the distribution of our dependent variable (revenue)

```{r}
library(dplyr)
data$quartile <- ntile(data$revenue, 5)
size = as.matrix(table(data$quartile))
cbind(aggregate(data$revenue, by = list(data$quartile), mean), size)
ggplot(data, aes(x = revenue)) + geom_histogram()

```

Plot of budget & revenue
```{r}
plot(data$budget, data$revenue)
```

I see a few outliers. But first I will impute the missing values and then remove the outliers if needed. 

Missing value estimation:
```{r}
mean_impute <- function(x){
  a<- (mean(x[!is.na(x)]))
  x <- ifelse(is.na(x), a, x)
  return(x)
}

median_impute <-  function(x){
  a<- (median(x[!is.na(x)]))
  x <- ifelse(is.na(x), a, x)
  return(x)
}

mode_impute <-  function(x){
  ux <- (unique(x))
  a<-ux[which.max(tabulate(match(x[!is.na(x)], ux)))]
  x <- ifelse(is.na(x), a, x)
  return(x)
}

data.imp <-data

#Imputing missing data in gender and runtime columns using mode and median respectively

data.imp$actor_1_gender <- as.factor(mode_impute(data$actor_1_gender))
data.imp$actor_2_gender <- as.factor(mode_impute(data$actor_2_gender))
data.imp$runtime <- median_impute(data.imp$runtime)

perc_blank <- function(x){
  return(sum(x ==""|x==" ")/length(x))
}
round(apply(data.imp, 2, function(x) perc_blank(x)),2)

```

Removing the extreme revenue records
```{r}
sd.budget <- sqrt(var(data.imp$budget))
sd.budget*6
length(data.imp$budget[data.imp$budget>2e+08])
data.imp <- data.imp[data.imp$budget<2e+08,]
```

```{r}
data.final <-data.imp[,-which(names(data.imp) %in% c( "actor_2_gender", "actor_3_gender", "actor_4_gender", "actor_5_gender",  "director_gender", "producer_gender", 'na_count', 'genre_1', 'adult','tagline','original_language'))]
data.final$success <- ifelse(data.final$revenue>1.25*(data.final$budget),1,0)

#Splitting the data into test & train
c <- round(nrow(data.final)*0.7,0)
s <- sample(1:nrow(data.final), c)

train <- data.final[s,]
test <- data.final[-s,]
```

Building a model to predict revenue of the movie before it is released. I will not model the vote count and vote average variables as they are collected after the release of the movie.

Build a multiple linear model for revenue prediction

```{r}
par(mfrow = c(2,3))
for (i in c("budget","popularity","runtime", "vote_average", "vote_count")){
  plot(data.final$revenue, eval(parse(text = paste0("data.final$",i))), ylab = i)
}
```

Cleary vote count has an increasing relationship with revenue as that indicates # people who went to watch the movie. But I cannot model vote average and vote count for predicting revenue as they are collected after the movie is released

**Fitting a linear model**
```{r}

revenue_pred <- lm(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+num_prod_ctry + budget, data = train)
summary(revenue_pred)

library(Metrics)

rmseTest <- rmse((predict(revenue_pred, test)),test$revenue)
rmseTrain <-rmse((predict(revenue_pred, train)),train$revenue)

#Validation
cbind(rmseTrain, rmseTest)
```

The linear model does not five a good fit and the resons could be:
+ interaction between variables e.g.: popularity is driven by actors, budget is driven by runtime- number of countries in which the movie is launched
+ Scatter plots do not show a perfect linear relationship between the dependent and independent variables

**Fitting a regression tree model**
to take into account the interactions between variables
```{r}
library(rpart)
library(rpart.plot)

revenue_tree <- rpart(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+num_prod_ctry + budget , data = train)

plotcp(revenue_tree)

# Retreive optimal cp value based on cross-validated error
opt_index <- which.min(revenue_tree$cptable[, "xerror"])
cp_opt <- revenue_tree$cptable[opt_index, "CP"]

revenue_tree_opt <- prune(tree = revenue_tree, 
                         cp = cp_opt)

# Display the pruned tree results
rpart.plot(x = revenue_tree_opt, yesno = 2, type = 1, extra = 1)
```

Looking at the accuracy and confusion matrix from tree model (test vs. train)
```{r}

predictedTest <- predict(revenue_tree_opt, test)
rmseTest <- rmse((test$revenue),predictedTest)
rmseTrain <- rmse((train$revenue),predict(revenue_tree_opt, train))

r_sqTest <- 1-rmseTest^2/var(test$revenue)
r_sqTr <- 1-rmseTrain^2/var(train$revenue)

a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(r_sqTr, r_sqTest)),2)
rownames(a)<- c("RMSE", "R-sq")
colnames(a)<- c("Train", "Test")
a

```

The fit of regression tree is similar to the linear regression model. I will fit random forests to see if the accuracy can be improved further by bootstrapping: 

```{r}
library(randomForest)
rfor1 <- randomForest(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+num_prod_ctry + budget, data= train)
```

```{r}
rfor1
rmseTest <- rmse(predict(rfor1, test), test$revenue)
rmseTrain <- rmse(predict(rfor1, train), train$revenue)
Rsq_test <- 1-(rmseTest)^2/var(test$revenue)
R_sqTr <- 1-(rmseTrain)^2/var(train$revenue)

a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(R_sqTr, Rsq_test)),2)
rownames(a)<- c("RMSE", "R-sq")
colnames(a)<- c("Train", "Test")
a

```

I see overfitting in this model. Let me set the node size to avoid this:
```{r}
for (i in c(20,25,30,35,40)){

  rfor2 <- randomForest(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data= train, nodesize = i)
  rmseTest <- rmse(predict(rfor2, test), test$revenue)
  rmseTrain <- rmse(predict(rfor2, train), train$revenue)
  Rsq_test <- 1-(rmseTest)^2/var(test$revenue)
  R_sqTr <- 1-(rmseTrain)^2/var(train$revenue)
  
  a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(R_sqTr, Rsq_test)),2)
  rownames(a)<- c("RMSE", "R-sq")
  colnames(a)<- c("Train", "Test")
  print(i)
  print(a)
}
```

I will go with the node size 25. The model fit has clearly improved with random forests compared to the tree model

```{r}
rfor.f <- randomForest(revenue~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data= train, nodesize = 25)
rfor.f$importance
```

**Performing cluster wise class regression**
```{r}

#clustreg function
clustreg=function(dat,k,tries,sed,niter){
  
  set.seed(sed)
  dat=as.data.frame(dat)
  rsq=rep(NA,niter)
  res=list()
  rsq.best=0
  for(l in 1:tries) {
    
    c = sample(1:k,nrow(dat),replace=TRUE)
    yhat=rep(NA,nrow(dat))
    for(i in 1:niter) {		
      resid=pred=matrix(0,nrow(dat),k)
      for(j in 1:k){	
        pred[,j]=predict(glm(dat[c==j,],family="gaussian"),newdata=dat)		
        resid[,j] = (pred[,j]-dat[,1])^2
      }
      
      c = apply(resid,1,which.min)
      for(m in 1:nrow(dat)) {yhat[m]=pred[m,c[m]]}
      rsq[i] = cor(dat[,1],yhat)^2	
      #print(rsq[i])
    }
    
    if(rsq[niter] > rsq.best) {	
      rsq.best=rsq[niter]
      l.best=l
      c.best=c
      yhat.best=yhat
    }
  }
  
  for(i in k:1) res[[i]]=summary(lm(dat[c.best==i,]))
  
  return(list(data=dat,nclust=k,tries=tries,seed=sed,rsq.best=rsq.best,number.loops=niter, Best.try=l.best,cluster=c.best,results=res))
}

clustreg.predict=function(results,newdat){
  
  yhat=rep(NA,nrow(newdat))
  resid=pred=matrix(0,nrow(newdat),length(table(results$cluster)))
  
  for(j in 1:length(table(results$cluster))){			
    pred[,j]=predict(glm(results$data[results$cluster==j,],family="gaussian"),newdata=newdat)		
    resid[,j] = (pred[,j]-newdat[,1])^2
  }
  
  c = apply(resid,1,which.min)
  for(m in 1:nrow(newdat)) {yhat[m]=pred[m,c[m]]}
  rsq = cor(newdat[,1],yhat)^2	
  
  return(list(results=results,newdata=newdat,cluster=c,yhat=yhat,rsq=rsq))
  
}
```

Trying multiple clusters and assessing fits using approximated R-sq and RMSE
```{r}

for (i in 2:4){
  
  rev_clust<- clustreg(train[,c(5,1,2,3,6,9,10,11)],i,100,881,50)
  ypredTr<- clustreg.predict(rev_clust, train[,c(5,1,2,3,6,9,10,11)])
  ypredTest <- clustreg.predict(rev_clust,test[,c(5,1,2,3,6,9,10,11)])
  yhat_tr<- ypredTr$yhat
  yhat_test<- ypredTest$yhat
  
  rmseTest <- rmse(yhat_test, test$revenue)
  rmseTrain <- rmse(yhat_tr, train$revenue)
  Rsq_test <- 1-(rmseTest)^2/var(test$revenue)
  R_sqTr <- 1-(rmseTrain)^2/var(train$revenue)
  
  a<- round(rbind(cbind(rmseTrain, rmseTest), cbind(R_sqTr, Rsq_test)),2)
  rownames(a)<- c("RMSE", "R-sq")
  colnames(a)<- c("Train", "Test")
  print(i)
  x <- table(rev_clust$cluster)
  print(x)
  print(a)
  
}

```

Cluster wise regression is fitting really well. Based on the size of the clusters, I will choose the 3 cluster solution and clusters get thin beyond that.

```{r}
rev_clust3<- clustreg(train[,c(5,1,2,3,6,9,10,11)],3,100,881,50)
rev_clust3$result
```
 
```{r}
train_clus <- as.data.frame(cbind(train, cluster = rev_clust3$cluster))
train_clus$cluster <- as.factor(train_clus$cluster)
cbind(aggregate(train_clus[,c(2,5,3,6,7,8,10)], by = list(train_clus$cluster), mean), size = table(train_clus$cluster))
```

Fitting a classification tree on train data to classify new datasets into clusters for revenue predictions through clusterwise regression results 
```{r}
s <- sample(1:nrow(train), round(0.7*nrow(train),0))
train1 <- train_clus[s,]
train2 <- train_clus[-s,]

  classify <- randomForest(cluster~ actor_1_gender+ popularity+runtime+collection+num_prod_comp+ budget, data= train1, nodesize = 10)
  train1clus <- predict(classify, train1, type = "class")
  train2clus <- predict(classify, train2, type = "class")
  round(prop.table(table(actual = train1$cluster, pred = train1clus),1),2)
  round(prop.table(table(actual = train2$cluster, pred = train2clus),1),2)
  
  accTrain1 <- sum(train1clus==train1$cluster)/nrow(train1); accTrain1
  accTrain2 <- sum(train2clus==train2$cluster)/nrow(train2); accTrain2

```
  
The accuracy of classification model for deciding clusters in low of small sized slusters. I would weight in this factor to decide between random forest and cluster wise regression result.  

